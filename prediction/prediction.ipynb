{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6b3f130",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "You may use this notebook to use our models to make predictions on arbitrary text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56666a5a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5153311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409cd761",
   "metadata": {},
   "source": [
    "## Load Models and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f166a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_input(text):\n",
    "    # Load all models and resources\n",
    "    meta_model = joblib.load(\"../stacking-models/meta_model.pkl\")\n",
    "    lr_model = joblib.load(\"../base-models/logistic_regression_model.pkl\")\n",
    "    rf_model = joblib.load(\"../base-models/random_forest_model.pkl\")\n",
    "    xgb_model = joblib.load(\"../base-models/gradient_boosted_model.pkl\")\n",
    "    svm_model = joblib.load(\"../base-models/support_vector_machine_model.pkl\")\n",
    "    lrbert_model = joblib.load(\"../base-models/lr_bert_model.pkl\")\n",
    "    tfidf_vectorizer = joblib.load(\"../datasets/tfidf_vectorizer.pkl\")\n",
    "    w2v_model = joblib.load(\"../datasets/word2vec_model.model\")\n",
    "    idf_weights = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_))\n",
    "\n",
    "    device = \"cpu\"\n",
    "    bert_model = DistilBertForSequenceClassification.from_pretrained(\"../base-models/finetuned_bert\")\n",
    "    bert_model.to(device)\n",
    "    bert_model.eval()\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"../base-models/finetuned_bert\")\n",
    "\n",
    "    # TF-IDF\n",
    "    tfidf_feat = tfidf_vectorizer.transform([text])\n",
    "\n",
    "    # Weighted Word2Vec\n",
    "    def get_weighted_w2v(text, model, idf_dict):\n",
    "        tokens = word_tokenize(text)\n",
    "        word_vecs = []\n",
    "        weight_sum = 0\n",
    "        for word in tokens:\n",
    "            if word in model.wv and word in idf_dict:\n",
    "                vec = model.wv[word] * idf_dict[word]\n",
    "                word_vecs.append(vec)\n",
    "                weight_sum += idf_dict[word]\n",
    "        if word_vecs:\n",
    "            return np.sum(word_vecs, axis=0) / weight_sum\n",
    "        else:\n",
    "            return np.zeros(model.vector_size)\n",
    "\n",
    "    w2v_feat = get_weighted_w2v(text, w2v_model, idf_weights).reshape(1, -1)\n",
    "    full_feat = np.hstack([tfidf_feat.toarray(), w2v_feat])\n",
    "\n",
    "    # BERT Embedding\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs, output_hidden_states=True)\n",
    "        logits = outputs.logits\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        mean_embedding = (sum_embeddings / sum_mask).cpu().numpy()\n",
    "\n",
    "    # Construct meta-features\n",
    "    meta_input = np.zeros((1, 6))\n",
    "    meta_input[:, 0] = lr_model.predict_proba(tfidf_feat)[:, 1]\n",
    "    meta_input[:, 1] = rf_model.predict_proba(tfidf_feat)[:, 1]\n",
    "    meta_input[:, 2] = xgb_model.predict_proba(full_feat)[:, 1]\n",
    "    meta_input[:, 3] = svm_model.decision_function(tfidf_feat)\n",
    "    meta_input[:, 4] = lrbert_model.predict_proba(mean_embedding)[:, 1]\n",
    "    meta_input[:, 5] = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "    # Predict with meta-model\n",
    "    prediction = meta_model.predict(meta_input)[0]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca6e45e",
   "metadata": {},
   "source": [
    "## Make predictions on arbitrary text\n",
    "\n",
    "Where 0 means the text is not describing a natural disaster and 1 means the text is describing a natural disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d512eeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted that text is describing a natural disaster: 1\n"
     ]
    }
   ],
   "source": [
    "prediction = predict_input(\"There is a wildfire raging in southern California.\")\n",
    "print(\"Predicted that text is describing a natural disaster:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c61a88aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted that text is describing a natural disaster: 0\n"
     ]
    }
   ],
   "source": [
    "prediction = predict_input(\"What a peaceful day!\")\n",
    "print(\"Predicted that text is describing a natural disaster:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec88ccaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
